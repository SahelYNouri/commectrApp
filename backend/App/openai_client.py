from dotenv import load_dotenv
import os
from openai import OpenAI

load_dotenv()
client = OpenAI(api_key= os.getenv("OPENAI_API_KEY"))


# build full prompt text from the profile dict and goal
def generate_cold_message(profile: dict, goal_prompt: str) ->str:
    
    prompt = f"""
You are an expert at writing concise, and personalized Linkedin cold messages 

Target person: 
- Name: {profile.get('target_name')}
- Role: {profile.get('target_role')}
- Company: {profile.get('company')}
- LinkedIn: {profile.get('linkedin_url')}
- Experiences: {profile.get('experiences')}
- Recent Post: {profile.get('recent_post')}
- Education: {profile.get('education')}
- Other notes: {profile.get('other_notes')}

User's goal:
 {goal_prompt}

Write a friendly, human, 4-7 sentence LinkedIn DM. Do NOT add any explanations, only the message text.
    """
    #calling the chat completions endpoint of the OpenAI APi with both a system and user message.
    resp= client.chat.completions.create(
        model= "gpt-5-nano-2025-08-07",
        messages= [
            {"role": "system", "content": "you write concise, warm Linkedin DMs."},
            {"role": "user", "content": prompt},
        ],
    )

    #return the first response generated by the model, stips extra whitespace
    return resp.choices[0].message.content.strip()

